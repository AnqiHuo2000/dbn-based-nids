{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98645857",
   "metadata": {},
   "source": [
    "# Building a deep belief network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc106553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from data import cicids\n",
    "from utils import utils\n",
    "from models import DBN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846ad12e",
   "metadata": {},
   "source": [
    "Check if GPU is active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99e71fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4535e75",
   "metadata": {},
   "source": [
    "### Create the DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1aa2a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# instances in training set:  1318783\n",
      "# instances in validation set:  439595\n",
      "# instances in testing set:  439595\n"
     ]
    }
   ],
   "source": [
    "# Get the datasets\n",
    "train_data, val_data, test_data = cicids.get_dataset()\n",
    "\n",
    "# How many instances have we got?\n",
    "print('# instances in training set: ', len(train_data))\n",
    "print('# instances in validation set: ', len(val_data))\n",
    "print('# instances in testing set: ', len(test_data))\n",
    "\n",
    "samples_weight = utils.get_samples_weight(train_data.labels['label'])\n",
    "sampler = torch.utils.data.sampler.WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# Create the dataloaders - for training, validation and testing\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=batch_size, sampler=sampler)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset=val_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = torch.utils.data.DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebedc4c3",
   "metadata": {},
   "source": [
    "### Instantiate the network, the loss function and the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a1eb748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some input variables\n",
    "n_classes = 6\n",
    "num_epochs = 1\n",
    "tag = \"\"\n",
    "\n",
    "# Creating a DBN\n",
    "model = DBN(n_visible=47,\n",
    "            n_hidden=(128, 128),\n",
    "            k=(1, 1),\n",
    "            learning_rate=(0.1, 0.1),\n",
    "            momentum=(0, 0),\n",
    "            decay=(0, 0), \n",
    "            batch_size=[64, 64],\n",
    "            num_epochs=[5, 5],\n",
    "            device=device)\n",
    "\n",
    "# Training a DBN\n",
    "# model.fit(train_loader)\n",
    "\n",
    "# Creating the optimzers\n",
    "optimizer = [optim.Adam(m.parameters(), lr=0.001) for m in model.models]\n",
    "optimizer.append(optim.Adam(model.fc.parameters(), lr=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2556ac55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBN(\n",
      "  (fc): Linear(in_features=128, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316a63d6",
   "metadata": {},
   "source": [
    "### Train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08db763",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: torch.nn.ModuleList,\n",
    "    optimizer: torch.optim,\n",
    "    train_loader: torch.utils.data.DataLoader,\n",
    "    valid_loader: torch.utils.data.DataLoader,\n",
    "    num_epochs: int,\n",
    "    device: torch.device,\n",
    "    tag=''\n",
    "):\n",
    "\n",
    "    # Cross-Entropy loss is used for the discriminative fine-tuning\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    history = {\n",
    "        'train': {\n",
    "            'total': 0,\n",
    "            'loss': [],\n",
    "            'accuracy': [],\n",
    "            'output_pred': [],\n",
    "            'output_true': []\n",
    "        },\n",
    "        'validation': {\n",
    "            'total': 0,\n",
    "            'loss': [],\n",
    "            'accuracy': [],\n",
    "            'output_pred': [],\n",
    "            'output_true': []\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "\n",
    "        ##################################\n",
    "        ##          TRAIN LOOP          ##\n",
    "        ##################################\n",
    "        model.train()\n",
    "\n",
    "        train_loss = 0.0\n",
    "        train_steps = 0\n",
    "        train_total = 0\n",
    "        train_correct = 0\n",
    "\n",
    "        train_output_pred = []\n",
    "        train_output_true = []\n",
    "\n",
    "        # For every possible batch\n",
    "        print(f\"{tag} Epoch {epoch}/{num_epochs}:\")\n",
    "        for inputs, labels in tqdm(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            for opt in optimizer:\n",
    "                opt.zero_grad()\n",
    "\n",
    "            # Passing the batch down the model\n",
    "            outputs = model(inputs.float())\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            # For every possible optimizer performs the gradient update\n",
    "            for opt in optimizer:\n",
    "                opt.step()\n",
    "\n",
    "            train_loss += loss.cpu().item()\n",
    "            train_steps += 1\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            train_output_pred += outputs.argmax(1).tolist()\n",
    "            train_output_true += labels.tolist()\n",
    "\n",
    "\n",
    "        ############################################\n",
    "        ##               VALID LOOP               ##\n",
    "        ############################################\n",
    "        model.eval()\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        val_total = 0\n",
    "        val_correct = 0\n",
    "\n",
    "        val_output_pred = []\n",
    "        val_output_true = []\n",
    "\n",
    "        for inputs, labels in valid_loader:\n",
    "            with torch.no_grad():\n",
    "\n",
    "                # Passing the batch down the model\n",
    "                outputs = model(inputs.float())\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.cpu().item()\n",
    "                val_steps += 1\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "                val_output_pred += outputs.argmax(1).tolist()\n",
    "                val_output_true += labels.tolist()\n",
    "\n",
    "        history['train']['total'] = train_total\n",
    "        history['train']['loss'].append(train_loss/train_steps)\n",
    "        history['train']['accuracy'].append(train_correct/train_total)\n",
    "        history['train']['output_pred'] = train_output_pred\n",
    "        history['train']['output_true'] = train_output_true\n",
    "\n",
    "        history['validation']['total'] = val_total\n",
    "        history['validation']['loss'].append(val_loss/val_steps)\n",
    "        history['validation']['accuracy'].append(val_correct/val_total)\n",
    "        history['validation']['output_pred'] = val_output_pred\n",
    "        history['validation']['output_true'] = val_output_true\n",
    "\n",
    "        print(f'{tag} loss: {train_loss/train_steps} - acc: {train_correct/train_total} - val_loss: {val_loss/val_steps} - val_acc: {val_correct/val_total}\\n')\n",
    "    \n",
    "    print(f\"{tag} Finished Training\")\n",
    "    return history, train_loss/train_steps, train_correct/train_total, val_loss/val_steps, val_correct/val_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d29cfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "history, _, _, _, _ = train(model, optimizer, train_loader, valid_loader, num_epochs, device)\n",
    "\n",
    "training_loss = history['train']['loss']\n",
    "training_accuracy = history['train']['accuracy']\n",
    "train_output_true = history['train']['output_true']\n",
    "train_output_pred = history['train']['output_pred']\n",
    "\n",
    "validation_loss = history['validation']['loss']\n",
    "validation_accuracy = history['validation']['accuracy']\n",
    "valid_output_true = history['validation']['output_true']\n",
    "valid_output_pred = history['validation']['output_pred']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92503fa",
   "metadata": {},
   "source": [
    "### Plot loss vs iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5efa9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 8))\n",
    "plt.plot(training_loss, label='train - loss')\n",
    "plt.plot(validation_loss, label='validation - loss')\n",
    "plt.title(\"Train and Validation Loss\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "plt.plot(training_accuracy, label='train - accuracy')\n",
    "plt.plot(validation_accuracy, label='validation - accuracy')\n",
    "plt.title(\"Train and Validation Accuracy\")\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7163a17",
   "metadata": {},
   "source": [
    "### Plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2ce73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Benign', 'Botnet ARES', 'Brute Force', 'DoS/DDoS', 'PortScan', 'Web Attack']\n",
    "\n",
    "utils.plot_confusion_matrix(y_true=train_output_true,\n",
    "                            y_pred=train_output_pred,\n",
    "                            labels=labels,\n",
    "                            title=\"Training Set - Normalized confusion matrix\",\n",
    "                            save=True,\n",
    "                            filename=\"dbn_train_confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6625aa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"Training Set -- Classification Report\", end=\"\\n\\n\")\n",
    "print(classification_report(train_output_true, train_output_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcff472",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_confusion_matrix(y_true=valid_output_true,\n",
    "                      y_pred=valid_output_pred,\n",
    "                      labels=labels,\n",
    "                      title=\"Validation Set - Normalized confusion matrix\",\n",
    "                      save=True,\n",
    "                      filename=\"dbn_valid_confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a023b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Validation Set -- Classification Report\", end=\"\\n\\n\")\n",
    "print(classification_report(valid_output_true, valid_output_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1fc44c",
   "metadata": {},
   "source": [
    "### Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924ef83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(\n",
    "    model: torch.nn.ModuleList,\n",
    "    test_loader: torch.utils.data.DataLoader,\n",
    "    device: torch.device,\n",
    "):\n",
    "    \"\"\"Validate the network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: torch.nn.ModuleList\n",
    "        Neural network model used in this example.\n",
    "\n",
    "    test_loader: torch.utils.data.DataLoader\n",
    "        DataLoader used in testing.\n",
    "\n",
    "    device: torch.device\n",
    "        (Default value = torch.device(\"cpu\"))\n",
    "        Device where the network will be trained within a client.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        Tuple containing the history, and a detailed report.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    history = {\n",
    "        'test': {\n",
    "            'total': 0,\n",
    "            'loss': 0.0,\n",
    "            'accuracy': 0.0,\n",
    "            'output_pred': [],\n",
    "            'output_true': [],\n",
    "            'output_pred_prob': []\n",
    "        }\n",
    "    }\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    test_loss = 0.0\n",
    "    test_steps = 0\n",
    "    test_total = 0\n",
    "    test_correct = 0\n",
    "\n",
    "    test_output_pred = []\n",
    "    test_output_true = []\n",
    "    test_output_pred_prob = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (inputs, labels) in tqdm(test_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs.float())\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            test_loss += loss.cpu().item()\n",
    "            test_steps += 1\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            test_output_pred += outputs.argmax(1).cpu().tolist()\n",
    "            test_output_true += labels.tolist()\n",
    "            test_output_pred_prob += nn.functional.softmax(outputs, dim=0).cpu().tolist()\n",
    "\n",
    "    history['test']['total'] = test_total\n",
    "    history['test']['loss'] = test_loss/test_steps\n",
    "    history['test']['accuracy'] = test_correct/test_total\n",
    "    history['test']['output_pred'] = test_output_pred\n",
    "    history['test']['output_true'] = test_output_true\n",
    "    history['test']['output_pred_prob'] = test_output_pred_prob\n",
    "\n",
    "    print(f'Test loss: {test_loss/test_steps}, Test accuracy: {test_correct/test_total}')\n",
    "\n",
    "    report = classification_report(\n",
    "        y_true=history['test']['output_true'],\n",
    "        y_pred=test_output_pred,\n",
    "        zero_division=0,\n",
    "        target_names=['Benign', 'Botnet ARES', 'Brute Force', 'DoS/DDoS', 'PortScan', 'Web Attack'],\n",
    "        output_dict=True,\n",
    "        digits=4,\n",
    "    )\n",
    "\n",
    "    return history, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f52874",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "### TEST LOOP ###\n",
    "#################\n",
    "history, report = test(model, test_loader, device)\n",
    "\n",
    "test_output_pred = history['test']['output_pred']\n",
    "test_output_pred_prob = history['test']['output_pred_prob']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d6fa67",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cbf5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_confusion_matrix(y_true=test_data.labels['label'].tolist(),\n",
    "                            y_pred=test_output_pred,\n",
    "                            labels=labels,\n",
    "                            title=\"Testing Set - Normalized confusion matrix\",\n",
    "                            save=True,\n",
    "                            filename=\"dbn_test_confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379f62e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing Set -- Classification Report\", end=\"\\n\\n\")\n",
    "print(classification_report(test_data.labels['label'].tolist(), test_output_pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9826140",
   "metadata": {},
   "source": [
    "### Plot ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee52a0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test_data.labels['label'].tolist()\n",
    "y_test = pd.get_dummies(y_test).values\n",
    "y_score = np.array(test_output_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2b109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_roc_curve(y_test=y_test,\n",
    "                     y_score=y_score,\n",
    "                     labels=labels,\n",
    "                     save=True,\n",
    "                     filename=\"dbn_roc_curve.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8a8004",
   "metadata": {},
   "source": [
    "### Plot Precision vs. Recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0553b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_precision_recall_curve(y_test,\n",
    "                                  y_score,\n",
    "                                  labels=labels,\n",
    "                                  save=True,\n",
    "                                  filename=\"dbn_prec_recall_curve.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abdf616",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8453b3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../checkpoints/wrs_deep_belief_network.pt'\n",
    "torch.save({\n",
    "            'epoch': num_epochs,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            # 'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, path)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
