{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe85000d",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning with Ray Tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a7671d",
   "metadata": {},
   "source": [
    "### Setup / Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d48a9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "from data import cicids\n",
    "from models import DBN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "970fef3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None, 'max_colwidth', None, 'display.expand_frame_repr', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbc18f7",
   "metadata": {},
   "source": [
    "### The train function\n",
    "We wrap the training script in a function `train(config, checkpoint_dir=None, data_dir=None`. As you can guess, the config parameter will receive the hyperparameters we would like to train with. The checkpoint_dir parameter is used to restore checkpoints. The data_dir specifies the directory where we load and store the data, so multiple runs can share the same data source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24944e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, checkpoint_dir=None, data_dir=None):\n",
    "    net = DBN(n_hidden=config[\"n_hidden\"], \n",
    "              learning_rate=config[\"learning_rate_rbm\"], \n",
    "              batch_size=config[\"batch_size_rbm\"])\n",
    "\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            net = nn.DataParallel(net)\n",
    "    net.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = [optim.Adam(m.parameters(), lr=config[\"learning_rate_dbn\"]) for m in net.models]\n",
    "    optimizer.append(optim.Adam(net.fc.parameters(), lr=config[\"learning_rate_dbn\"]))\n",
    "\n",
    "    if checkpoint_dir:\n",
    "        model_state, optimizer_state = torch.load(os.path.join(checkpoint_dir, \"checkpoint\"))\n",
    "        net.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "    train_data, val_data, test_data = cicids.get_dataset()\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_data,\n",
    "        batch_size=int(config[\"batch_size_dbn\"]),\n",
    "        shuffle=True,\n",
    "    )\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        dataset=val_data,\n",
    "        batch_size=int(config[\"batch_size_dbn\"]),\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "\n",
    "    for epoch in range(1, 10+1):\n",
    "\n",
    "        ##################################\n",
    "        ##          TRAIN LOOP          ##\n",
    "        ##################################\n",
    "        net.train()\n",
    "\n",
    "        train_loss = 0.0\n",
    "        train_steps = 0\n",
    "        train_total = 0\n",
    "        train_correct = 0\n",
    "\n",
    "        train_output_pred = []\n",
    "        train_output_true = []\n",
    "\n",
    "        print(f\"{tag} Epoch {epoch}/{num_epochs}:\")\n",
    "        for inputs, labels in tqdm(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            for opt in optimizer:\n",
    "                opt.zero_grad()\n",
    "\n",
    "            # Passing the batch down the model\n",
    "            outputs = net(inputs.float())\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            # For every possible optimizer performs the gradient update\n",
    "            for opt in optimizer:\n",
    "                opt.step()\n",
    "\n",
    "            train_loss += loss.cpu().item()\n",
    "            train_steps += 1\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            train_output_pred += outputs.argmax(1).tolist()\n",
    "            train_output_true += labels.tolist()\n",
    "\n",
    "        ############################################\n",
    "        ##               VALID LOOP               ##\n",
    "        ############################################\n",
    "        net.eval()\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        val_steps = 0\n",
    "        val_total = 0\n",
    "        val_correct = 0\n",
    "\n",
    "        val_output_pred = []\n",
    "        val_output_true = []\n",
    "\n",
    "        for inputs, labels in valid_loader:\n",
    "            with torch.no_grad():\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = net(inputs.float())\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.cpu().item()\n",
    "                val_steps += 1\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "                val_output_pred += outputs.argmax(1).tolist()\n",
    "                val_output_true += labels.tolist()\n",
    "\n",
    "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "            torch.save((net.state_dict(), optimizer.state_dict()), path)\n",
    "\n",
    "        tune.report(train_loss=(train_loss/train_steps),\n",
    "                    train_accuracy=(train_correct/train_total),\n",
    "                    val_loss=(val_loss/val_steps),\n",
    "                    val_accuracy=(val_correct/val_total))\n",
    "        \n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecc7c80",
   "metadata": {},
   "source": [
    "### Test set accuracy\n",
    "Commonly the performance of a machine learning model is tested on a hold-out test\n",
    "set with data that has not been used for training the model. We also wrap this in a\n",
    "function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6f4f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(net, batch_size, device=\"cpu\"):\n",
    "    train_data, val_data, test_data = cicids.get_dataset()\n",
    "\n",
    "    test_loader  = torch.utils.data.DataLoader(\n",
    "        dataset=test_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "    history = {\n",
    "        'test': {\n",
    "            'total': 0,\n",
    "            'loss': 0.0,\n",
    "            'accuracy': 0.0,\n",
    "            'output_pred': [],\n",
    "            'output_true': [],\n",
    "            'output_pred_prob': []\n",
    "        }\n",
    "    }\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    test_loss = 0.0\n",
    "    test_steps = 0\n",
    "    test_total = 0\n",
    "    test_correct = 0\n",
    "\n",
    "    test_output_pred = []\n",
    "    test_output_true = []\n",
    "    test_output_pred_prob = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (inputs, labels) in tqdm(test_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = net(inputs.float())\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            test_loss += loss.cpu().item()\n",
    "            test_steps += 1\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            test_output_pred += outputs.argmax(1).cpu().tolist()\n",
    "            test_output_true += labels.tolist()\n",
    "            test_output_pred_prob += nn.functional.softmax(outputs, dim=0).cpu().tolist()\n",
    "            \n",
    "    return correct / totalcicids."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f75c9d4",
   "metadata": {},
   "source": [
    "### Configuring the search space\n",
    "\n",
    "Lastly, we need to define Ray Tune's search space.\n",
    "\n",
    "At each trial, Ray Tune will now randomly sample a combination of parameters from these search spaces. It will then train a number of models in parallel and find the best performing one among these. We also use the ASHAScheduler which will terminate bad performing trials early.\n",
    "\n",
    "You can specify the number of CPUs, which are then available e.g. to increase the num_workers of the PyTorch DataLoader instances. The selected number of GPUs are made visible to PyTorch in each trial. Trials do not have access to GPUs that haven't been requested for them - so you don't have to care about two trials using the same set of resources.\n",
    "\n",
    "Here we can also specify fractional GPUs, so something like gpus_per_trial=0.5 is completely valid. The trials will then share GPUs among each other. You just have to make sure that the models still fit in the GPU memory.\n",
    "After training the models, we will find the best performing one and load the trained network from the checkpoint file. We then obtain the test set accuracy and report everything by printing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c651822b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples=10\n",
    "max_num_epochs=10\n",
    "\n",
    "l = np.random.randint(1, 3)\n",
    "\n",
    "config = {\n",
    "    \"n_hidden\": [tune.sample_from(lambda _: 2**np.random.randint(2, 8)) for _ in range(l)],\n",
    "    \"learning_rate_rbm\": [tune.loguniform(1e-3, 1e-1) for _ in range(l)],\n",
    "    \"learning_rate_dbn\": tune.loguniform(1e-3, 1e-1),\n",
    "    \"batch_size_rbm\": [tune.choice([64, 128, 256, 512, 1024]) for _ in range(l)],\n",
    "    \"batch_size_dbn\": tune.choice([64, 128, 256, 512, 1024]),\n",
    "}\n",
    "scheduler = ASHAScheduler(\n",
    "    metric=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    max_t=max_num_epochs,\n",
    "    grace_period=1,\n",
    "    reduction_factor=2\n",
    ")\n",
    "reporter = CLIReporter(metric_columns=[\"train_loss\", \"train_accuracy\", \"val_loss\", \"val_accuracy\", \"training_iteration\"])\n",
    "result = tune.run(\n",
    "    train,\n",
    "    config=config,\n",
    "    num_samples=num_samples,\n",
    "    scheduler=scheduler,\n",
    "    progress_reporter=reporter\n",
    ")\n",
    "\n",
    "best_trial = result.get_best_trial(\"val_loss\", \"min\", \"last\")\n",
    "print(\"Best trial config: {}\".format(best_trial.config))\n",
    "print(\"Best trial final training loss: {}\".format(\n",
    "    best_trial.last_result[\"train_loss\"]))\n",
    "print(\"Best trial final training accuracy: {}\".format(\n",
    "    best_trial.last_result[\"train_accuracy\"]))\n",
    "print(\"Best trial final validation loss: {}\".format(\n",
    "    best_trial.last_result[\"val_loss\"]))\n",
    "print(\"Best trial final validation accuracy: {}\".format(\n",
    "    best_trial.last_result[\"val_accuracy\"]))\n",
    "\n",
    "best_trained_model = NeuralNetwork(best_trial.config[\"hidden1_size\"],\n",
    "                                   best_trial.config[\"hidden2_size\"],\n",
    "                                   best_trial.config[\"hidden3_size\"])\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "    if gpus_per_trial > 1:\n",
    "        best_trained_model = nn.DataParallel(best_trained_model)\n",
    "best_trained_model.to(device)\n",
    "\n",
    "best_checkpoint_dir = best_trial.checkpoint.value\n",
    "model_state, optimizer_state = torch.load(os.path.join(\n",
    "    best_checkpoint_dir, \"checkpoint\"))\n",
    "best_trained_model.load_state_dict(model_state)\n",
    "\n",
    "test_acc = test_accuracy(best_trained_model, best_trial.config[\"batch_size\"], device)\n",
    "print(\"Best trial test set accuracy: {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995fc245",
   "metadata": {},
   "source": [
    "| Trial name        | status     | loc   |   batch_size |   hidden1_size |   hidden2_size |   hidden3_size |         lr |   train_loss |   train_accuracy |   val_loss |   val_accuracy |   training_iteration |\n",
    "|-------------------|------------|-------|--------------|----------------|----------------|----------------|------------|--------------|------------------|------------|----------------|----------------------|\n",
    "| 00000 | TERMINATED |       |          128 |              4 |             16 |             32 | 0.0499089  |    0.0965664 |         0.969098 |  0.216287  |       0.915488 |                    2 |\n",
    "| 00001 | TERMINATED |       |          256 |              4 |              4 |              8 | 0.0146023  |    0.0738336 |         0.977657 |  0.205247  |       0.930971 |                   10 |\n",
    "| 00002 | TERMINATED |       |          512 |              4 |            128 |             32 | 0.00616173 |    0.308407  |         0.894283 |  0.289828  |       0.904676 |                    1 |\n",
    "| 00003 | TERMINATED |       |          256 |             16 |            128 |             64 | 0.0171306  |    0.0174327 |         0.994842 |  0.0425004 |       0.986367 |                   10 |\n",
    "| 00004 | TERMINATED |       |          256 |              8 |             16 |            128 | 0.0815692  |    0.0346922 |         0.988216 |  0.0969654 |       0.966947 |                   10 |\n",
    "| 00005 | TERMINATED |       |           64 |              8 |             16 |             16 | 0.0365723  |    0.0593174 |         0.980345 |  0.233777  |       0.925306 |                    2 |\n",
    "| 00006 | TERMINATED |       |          256 |             32 |            128 |             32 | 0.00677515 |    0.0332826 |         0.989082 |  0.0936671 |       0.960486 |                    8 |\n",
    "| 00007 | TERMINATED |       |           64 |              4 |             64 |             64 | 0.00243203 |    0.174005  |         0.944215 |  0.254956  |       0.906671 |                    1 |\n",
    "| 00008 | TERMINATED |       |          512 |             32 |             64 |             16 | 0.0652585  |    0.0168181 |         0.994624 |  0.0560469 |       0.982534 |                   10 |\n",
    "| 00009 | TERMINATED |       |          128 |            128 |              8 |              8 | 0.0820741  |    0.0271389 |         0.991193 |  0.0768982 |       0.97436  |                   10 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
